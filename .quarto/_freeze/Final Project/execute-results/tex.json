{
  "hash": "00cc275ff35695ff0ee96c96a716fd2e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Final Project: Data Science Culmination Project\"\nauthor: \"Josh Miller & Elijah Hill\"\nformat: pdf\n---\n\n\n\\noindent Project Proposal: Week of November 13-17th\n\n\\noindent Project Due: Our Final Exam Time.\n\n## Assignment Description\n\nThis is it! This is the culmination of all your work in both of the data science courses! The parameters of this project are very general because I want to give you the chance to explore your interests and be creative. Generally, your project will go through the entire data science process. The project will involve a formal written document as well as a presentation. The written document will be worth 2/3rd's of the final grade and the presentation will be worth the other 1/3rd.\n\nHere are the sections on how the project will be evaluated (A rubric will be released later with more specific parameters).\n\n-   Questions and Goals: The questions you wish to answer and the goals of the project. You should have multiple questions that you answer in the project. Not all of them need to be questions that require modeling to answer, but some of them need to be.\n-   Data Acquisition: The project describes how the data was obtained and gives substantial backround on the data.\n-   Data Preprocessing: Throughout the project, the proper preprocessing techniques (variable transformations, reshaping data, etc.) are utilized.\n-   Exploratory Data Analysis: Proper exploratory plots and summarizes are utilized to describe the data and showcase certain interesting aspects of the data that you will explore later in the project.\n-   Modeling and Analysis: This is a large portion of the project! You work toward answer the questions/goals you stated at the beginning. Your project needs to include at least 3 modeling techniques we discussed in class. You will fit, tune, and compare the methods. You will discuss the results and why they make sense in context. This section can include a wide range of modeling techniques. Your proposal should be focused on describing what you want to do in this section.\n-   Data Product: You present your data, models, and conclusions in a professional manner. This could include an interactive data product.\n\n## Place Work Below!!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.4.4      v purrr   1.0.2 \nv tibble  3.1.8      v dplyr   1.0.10\nv tidyr   1.3.0      v stringr 1.5.1 \nv readr   2.1.3      v forcats 0.5.2 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggplot2' was built under R version 4.2.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'tidyr' was built under R version 4.2.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'purrr' was built under R version 4.2.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'stringr' was built under R version 4.2.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n```\n\n\n:::\n\n```{.r .cell-code}\nstyled <-\n  theme_bw() + \n  theme(\n    plot.title = element_text(face = \"bold\", size = 12),\n    legend.background = element_rect(\n      fill = \"white\", \n      linewidth = 4, \n      colour = \"white\"\n    ),\n    axis.ticks = element_line(colour = \"grey70\", linewidth = 0.2),\n    panel.grid.major = element_line(colour = \"grey70\", linewidth = 0.2),\n    panel.grid.minor = element_blank()\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"tidymodels\") ; theme_set(styled)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching packages -------------------------------------- tidymodels 1.0.0 --\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nv broom        1.0.1     v rsample      1.1.0\nv dials        1.0.0     v tune         1.0.1\nv infer        1.0.3     v workflows    1.1.0\nv modeldata    1.0.1     v workflowsets 1.0.0\nv parsnip      1.0.2     v yardstick    1.1.0\nv recipes      1.0.2     \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Conflicts ----------------------------------------- tidymodels_conflicts() --\nx scales::discard() masks purrr::discard()\nx dplyr::filter()   masks stats::filter()\nx recipes::fixed()  masks stringr::fixed()\nx dplyr::lag()      masks stats::lag()\nx yardstick::spec() masks readr::spec()\nx recipes::step()   masks stats::step()\n* Use tidymodels_prefer() to resolve common conflicts.\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(\"janitor\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'janitor'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(\"olsrr\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'olsrr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:datasets':\n\n    rivers\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(\"doParallel\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: foreach\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'foreach'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:purrr':\n\n    accumulate, when\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: iterators\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: parallel\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(\"dplyr\")\nlibrary(\"kernlab\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'kernlab' was built under R version 4.2.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'kernlab'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:scales':\n\n    alpha\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:purrr':\n\n    cross\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(\"rpart.plot\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'rpart.plot' was built under R version 4.2.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: rpart\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'rpart'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dials':\n\n    prune\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(\"glmnet\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Matrix\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'Matrix'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoaded glmnet 4.1-4\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(\"GGally\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'GGally' was built under R version 4.2.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(\"cowplot\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'cowplot' was built under R version 4.2.3\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(\"jtools\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'jtools' was built under R version 4.2.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'jtools'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:yardstick':\n\n    get_weights\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(\"caret\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'caret' was built under R version 4.2.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: lattice\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'caret'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:yardstick':\n\n    precision, recall, sensitivity, specificity\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:purrr':\n\n    lift\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nall_cores <- parallel::detectCores(logical = FALSE)\ncl <- makePSOCKcluster(all_cores)\nregisterDoParallel(cl)\n```\n:::\n\n\n## Introduction:\n\nFor our Final Project, the dataset we decided to use was titled Salary by Job Title and Country. We found the dataset from Kaggle.com.\n\n<https://www.kaggle.com/datasets/amirmahdiabbootalebi/salary-by-job-title-and-country/data>\n\nThe dataset creator sourced this data from reputable employment websites and surveys, leaving out names and companies to ensure privacy for both parties.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSalary <- read_csv(\"Salary.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 6684 Columns: 9\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (4): Gender, Job Title, Country, Race\ndbl (5): Age, Education Level, Years of Experience, Salary, Senior\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\nThere are 9 variables in the data, with 6684 observations. The variables are as follows: Age, Gender, Education Level, Job Title, Years of Experience, Salary, Country, Race, and Senior. Education level is encoded from 0-3, 0 meaning the employee has a high school diploma as their highest level of education, 1 meaning that they have a Bachelor's degree, 2 meaning they have a Master's, and 3 meaning they have a Doctorate's. The senior variable is a binary value indicating whether or not they have a senior-level position. Salary has been converted into USD for all countries for the sake of being on the same scale.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(Salary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 9\n    Age Gender `Education Level` `Job Title` Years~1 Salary Country Race  Senior\n  <dbl> <chr>              <dbl> <chr>         <dbl>  <dbl> <chr>   <chr>  <dbl>\n1    32 Male                   1 Software E~       5  90000 UK      White      0\n2    28 Female                 2 Data Analy~       3  65000 USA     Hisp~      0\n3    45 Male                   3 Manager          15 150000 Canada  White      1\n4    36 Female                 1 Sales Asso~       7  60000 USA     Hisp~      0\n5    52 Male                   2 Director         20 200000 USA     Asian      0\n6    29 Male                   1 Marketing ~       2  55000 USA     Hisp~      0\n# ... with abbreviated variable name 1: `Years of Experience`\n```\n\n\n:::\n:::\n\n\n# Questions and Goals:\n\nOur main question we wanted to answer was \"Can we accurately predict the salary of a job given the predictors in this data set\", those being Age, Senior, Country, Race, Job Title, Gender, and Education Level. We also wanted to explore the roles each of the predictors play in determining Salary. Some secondary questions we asked to determine this during our EDA were: \"Is one gender more often lower-paid than another?\", \"Does an increase in age usually lead to an increase in salary?\", \"How big a difference does a job being a senior position make on average to Salary?\", and more to go along with that: \"Are older people more likely to be the ones occupying senior positions?\". Whether or not the Education Level or Country of the job seems to give access to a higher salary were also questions we asked and found answers to.\n\n# **Preprocessing:**\n\nFor preprocessing, we quickly found 2 issues: First, we realized that certain job titles only appear once in the entire data set, one of the most notable being CEO. While this had one of the largest values for salary in the entire dataset, we realized that this would not only skew our EDA but would also cause problems for our testing and training splits later on. Therefore, we decided to drop them.\n\nWe then found an issue with values that were likely misreported within the dataset. Upon analyzing the bottom-most values for annual salary in the dataset, we found multiple employees reported only making 3 figures with jobs that in every other case paid well above that, such as Software Engineer Manager. We could be making a large assumption here that this was a full-time position being paid a yearly salary, but even if these values were correctly recorded, it would still be inconsistent with the rest of the dataset and cause a skew in the lowest-paying jobs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##PREPROCESSING\n\n#removing any job only included once \nSalary_cleaned <- Salary %>%\n  group_by(`Job Title`) %>%\n  mutate(\n    count = n()\n  ) %>%\n  filter(count > 1) %>%\n  ungroup() %>%\n  select(-count)\n#dropping probable mistaken entries (reported less than 1k salaries)\nSalary_cleaned <- Salary_cleaned %>%\n  arrange(Salary) %>%\n  filter(!row_number() %in% c(1,2,3,4))\n```\n:::\n\n\n# **EDA:**\n\n## Exploring Gender:\n\nWe wanted to explore if Females still earned less than men on average, as they have historically, so we first looked at a general average of all salaries of men versus those of women.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#EDA (Gender)\n\n##GENDER DIFFERENCES\nSalary_cleaned_by_gender <- Salary_cleaned %>%\n  group_by(Gender) %>%\n    summarize(Mean = mean(Salary, na.rm = TRUE))\n#On average, women earn less than men\n\nSalary_cleaned_by_gender\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n  Gender    Mean\n  <chr>    <dbl>\n1 Female 107981.\n2 Male   121503.\n```\n\n\n:::\n:::\n\n\nThis table shows a sizable difference (about 19000) in the average salary of a male over one of a female, supporting our initial theory. We then split up the data to more deeply delve into the differences in pay between the two Genders.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Splitting salary into male and female\nsalary_male <- Salary_cleaned %>%\n  group_by(Gender) %>%\n    filter(Gender == \"Male\")\n\nsalary_female <- Salary_cleaned %>%\n  group_by(Gender) %>%\n    filter(Gender == \"Female\")\n```\n:::\n\n\nAfter splitting the data, we tried making four plots showing the top 15 highest-salary jobs and the bottom 15 lowest-salary jobs for comparison.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# #comparing highest/lowest earning male/female jobs\n#  top_male_salaries <- salary_male %>%\n#  arrange(desc(Salary)) %>%\n#    slice(1:15)\n#  \n#  #ignoring mistaken entries (1 and 2 row)\n#  bottom_male_salaries <- salary_male %>%\n#  arrange(Salary) %>%\n#    slice(3:17)\n#  \n#  top_female_salaries <- salary_female %>%\n#  arrange(desc(Salary)) %>%\n#  slice(1:15)\n#  \n# #ignoring mistaken entries (1 and 2 row)\n#  bottom_female_salaries <- salary_female %>%\n#  arrange(Salary) %>%\n#    slice(3:17)\n\n#plots\n#tms_plot <- ggplot(top_male_salaries, aes(x = Salary)) +       #geom_bar(fill = \"blue\") +\n#theme_light()\n\n#bms_plot <- ggplot(bottom_male_salaries, aes(x = Salary)) +\n# geom_bar(fill = \"turquoise2\") +\n# theme_dark()\n\n#the above plots don't look good...\n#they are mostly the same jobs\n\n#trying again but with...\n```\n:::\n\n\nthe above plots don't look good... they are mostly the same jobs trying again but with averaging jobs with the same title together.\n\nUpon making the first few plots, we realized that the above plots did not look good as they were mostly showing the same job titles' salaries repeated multiple times. We remade the graphs but this time combined the job titles to eliminate repeated Job Titles. First, we made new tables to use with a new Average_Salary column for each job title, then eliminated other columns and rows besides unique Average_Salaries and Job Titles since those were what we were focusing on.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#averaging jobs with the same title together\nsalary_male_unique <- salary_male %>%\n  group_by(`Job Title`) %>%\n  mutate(Average_Salary = mean(Salary)) %>%\n  distinct(Average_Salary)\n  \nsalary_female_unique <- salary_female %>%\n  group_by(`Job Title`) %>%\n  mutate(Average_Salary = mean(Salary)) %>%\n  distinct(Average_Salary)\n\nsalary_male_unique\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 61 x 2\n# Groups:   Job Title [61]\n   `Job Title`                    Average_Salary\n   <chr>                                   <dbl>\n 1 Sales Associate                        33515.\n 2 Delivery Driver                        28000 \n 3 Sales Representative                   46444.\n 4 Digital Marketing Manager              75968.\n 5 HR Generalist                          72776.\n 6 HR Coordinator                         34667.\n 7 Accountant                             53750 \n 8 Software Developer                     68011.\n 9 Business Development Associate         38333.\n10 Operations Analyst                     69167.\n# ... with 51 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nsalary_female_unique\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 67 x 2\n# Groups:   Job Title [67]\n   `Job Title`                     Average_Salary\n   <chr>                                    <dbl>\n 1 Sales Associate                         28207.\n 2 Sales Representative                    35833.\n 3 Receptionist                            25000 \n 4 HR Coordinator                          41062.\n 5 Customer Service Representative         33333.\n 6 HR Generalist                           48855.\n 7 Juniour HR Coordinator                  32000 \n 8 Marketing Analyst                       63083.\n 9 Business Development Associate          42500 \n10 Operations Manager                      95200 \n# ... with 57 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#comparing highest/lowest earning male/female jobs\ntop_male_salaries_unique <- \n  salary_male_unique %>%\n  ungroup() %>% arrange(desc(Average_Salary)) %>% slice(1:10)\n\nbottom_male_salaries_unique <- \n  salary_male_unique %>%\n  ungroup() %>% \n  arrange(Average_Salary) %>% \n  slice(1:10)\n\ntop_female_salaries_unique <- \n  salary_female_unique %>%\n  ungroup() %>% \n  arrange(desc(Average_Salary)) %>% slice(1:10)\n\nbottom_female_salaries_unique <- \n  salary_female_unique %>%\n  ungroup %>%\n  arrange(Average_Salary) %>%\n    slice(1:10)\n\ntop_male_salaries_unique \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 x 2\n   `Job Title`               Average_Salary\n   <chr>                              <dbl>\n 1 Director of Data Science         207742.\n 2 Marketing Director               189900 \n 3 Director of Engineering          180000 \n 4 Software Engineer Manager        173385.\n 5 Project Engineer                 173344.\n 6 Director of Operations           171667.\n 7 Director of Finance              170000 \n 8 Research Director                165870.\n 9 Data Scientist                   165062.\n10 Director of Marketing            160641.\n```\n\n\n:::\n\n```{.r .cell-code}\nbottom_male_salaries_unique\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 x 2\n   `Job Title`                    Average_Salary\n   <chr>                                   <dbl>\n 1 Delivery Driver                        28000 \n 2 Sales Associate                        33515.\n 3 HR Coordinator                         34667.\n 4 Business Operations Analyst            35000 \n 5 Business Development Associate         38333.\n 6 Juniour HR Generalist                  43000 \n 7 Sales Representative                   46444.\n 8 Sales Executive                        47083.\n 9 Graphic Designer                       51667.\n10 Accountant                             53750 \n```\n\n\n:::\n\n```{.r .cell-code}\ntop_female_salaries_unique\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 x 2\n   `Job Title`                 Average_Salary\n   <chr>                                <dbl>\n 1 Director of Data Science           200769.\n 2 Director of Human Resources        187500 \n 3 Director of Finance                180000 \n 4 Director of Operations             174000 \n 5 Product Manager                    172476.\n 6 Software Engineer Manager          171793.\n 7 Data Scientist                     162667.\n 8 Marketing Director                 162667.\n 9 Data Engineer                      160000 \n10 Research Director                  159310.\n```\n\n\n:::\n\n```{.r .cell-code}\nbottom_female_salaries_unique\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 x 2\n   `Job Title`                     Average_Salary\n   <chr>                                    <dbl>\n 1 Receptionist                            25000 \n 2 Sales Associate                         28207.\n 3 Juniour HR Coordinator                  32000 \n 4 Customer Service Representative         33333.\n 5 Sales Representative                    35833.\n 6 HR Coordinator                          41062.\n 7 Sales Executive                         41154.\n 8 Business Development Associate          42500 \n 9 Copywriter                              42500 \n10 Juniour HR Generalist                   43000 \n```\n\n\n:::\n:::\n\n\nWe made the plots again, making sure to standardize the x-axis values to more clearly show any differences in pay. We made male plots blue, and female red, top salary plots have a light theme, and bottom salary plots use the dark theme to differentiate and help show the comparisons we were looking for.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntms_plot <- ggplot(top_male_salaries_unique, aes(x = Average_Salary)) +\n    geom_histogram(fill = \"blue\") + \n    labs(y = \"Job Count\", x = \"Average Salary (Male)\") +\n    xlim(150000, 250000)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbms_plot <- ggplot(bottom_male_salaries_unique, aes(x = Average_Salary)) +\n    geom_histogram(fill = \"turquoise2\", bins = 40) +\n    labs(y = \"Job Count\", x = \"Average Salary (Male)\") +\n    xlim(24000, 58000)+ ylim(0, 3) + theme_dark()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntfs_plot <- ggplot(top_female_salaries_unique, aes(x = Average_Salary)) +\n    geom_histogram(fill = \"red\") +\n    labs(y = \"Job Count\", x = \"Average Salary (Female)\") +\n    xlim(150000, 250000)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbfs_plot <- ggplot(bottom_female_salaries_unique, aes(x = Average_Salary)) +\n    geom_histogram(fill = \"lightcoral\") +\n    labs(y = \"Job Count\", x = \"Average Salary (Female)\") +\n    xlim(24000, 58000) + ylim(0, 3) + theme_dark()\n```\n:::\n\n\nWe used the \"cowplot\" package to easily combine all four plots into one graphic for a more complete visual comparison of gender salary differences on the poles of the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_grid(tms_plot, bms_plot, tfs_plot, bfs_plot, nrow = 2, ncol = 2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 1 rows containing missing values (`geom_bar()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 1 rows containing missing values (`geom_bar()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Final-Project_files/figure-pdf/unnamed-chunk-12-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nAfter doing this, we realized that we could do almost the same thing, but in a broader sense (as well as faster), by just using a box plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#comparing all salaries\nggplot(Salary_cleaned, aes(x = Salary, y = Gender)) +\n    geom_boxplot(aes(\n        fill = as.factor(`Gender`))) +\n    scale_color_manual(values = c(\"blue\", \"red\")) +\n    theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](Final-Project_files/figure-pdf/unnamed-chunk-13-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThe box plots, as well as the four plots prior, all point to what we had guessed which was that males do indeed earn higher salaries than females on average.\n\n## Exploring Education Level:\n\nNext, we examined the role education level played on salary amount. This time, we started with the general plot comparing all salaries grouped by Education Level, then moved on to showing the average salary of each Education Level after that.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#EDA (Education Level)\n\nggplot(Salary_cleaned, aes(y = Salary,\n                           x = as.factor(`Education Level`),\n                           fill = as.factor(`Education Level`\n                                            )))+\n    scale_color_manual(values = c(\"red\", \"green\", \"yellow\", \"darkorchid3\")) +\n    labs(x = \"Education Level\") + \n    geom_boxplot() +\n    theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](Final-Project_files/figure-pdf/unnamed-chunk-14-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nsalary_by_ed_lvl <- Salary_cleaned %>%\n  group_by(`Education Level`) %>%\n  summarize(Mean = mean(Salary))\n\nggplot(salary_by_ed_lvl, aes(x = `Education Level`,\n                             y = Mean,\n                             fill = as.factor(\n                                 `Education Level`\n                                 ))) +\n    scale_color_manual(values = c(\"red\", \"green\", \"yellow\", \"darkorchid3\" )) + \n    labs(y = \"Mean Salary\") +\n    geom_col() +\n    theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](Final-Project_files/figure-pdf/unnamed-chunk-14-2.pdf){fig-pos='H'}\n:::\n:::\n\n\nWe were pleased to see not only that the data seemed to indicate that going to college is indeed still worth it, but that the data was nice and linear as well for both the raw and the average salary by education level comparisons.\n\n## Exploring Age and Seniority:\n\nAge and Seniority were two predictors we were especially excited to look at, and we had high expectations on the strength of the correlation between them and the salaries those of high age and in senior positions would hold. Once again, we showed a general plot using the raw salary data when compared with Age, this time using whether or not the job holder had the Senior status to determine the color of the plot point. After getting a nice-looking scatter plot from that (and being very happy with the color palette), we could see that there was some positive correlation between the age of a person, whether or not they would be in a senior position, and their salary. To get a slightly different perspective, we grouped the ages by decade and compared each Age Group's average salary to each other, and were once again satisfied to see a seemingly linear relationship between Age and Salary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#EDA (Age/Seniority)\nggplot(Salary_cleaned, aes(y = Salary, x = Age, color = as.factor(Senior))) +\n    scale_color_manual(values = c(\"ivory4\", \"goldenrod\"),\n                       labels = c(\"Non-Senior Position\", \n                                  \"Senior\")) +\n    labs(color = \"Seniority\") + geom_point()\n```\n\n::: {.cell-output-display}\n![](Final-Project_files/figure-pdf/unnamed-chunk-15-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nsalary_by_age <- Salary_cleaned %>%\n  mutate(Age_Group = case_when(\n    Age < 30 ~ \"29 & Younger\",\n    Age < 40 & Age >= 30 ~ \"30's\",\n    Age < 50 & Age >= 40 ~ \"40's\",\n    Age < 60 & Age >= 50 ~ \"50's\",\n    Age >= 60 ~ \"60 & Older\"\n  )) %>% \n  group_by(Age_Group) %>%\n  summarize(Average_Salary = mean(Salary))\n\nggplot(salary_by_age, aes(x = Age_Group,\n                          y = Average_Salary,\n                          fill = as.factor(Age_Group)),) +\n    scale_fill_manual(values = c(\"ivory4\",\"grey\",\n                                 \"lightgoldenrod2\",\n                                 \"goldenrod2\",\n                                 \"goldenrod3\")) +\n    geom_col() +\n    theme(legend.position = \"none\",\n                       panel.background = element_rect(fill = \"slategray3\")) +\n    labs(x = \"Age Group\", y = \"Average Salary\")\n```\n\n::: {.cell-output-display}\n![](Final-Project_files/figure-pdf/unnamed-chunk-15-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Exploring Country and Race:\n\nThe first thing we did to look at Country and Race was to use multilevel grouping to get a better understanding of the demographics of the data. After noting the variety of Races in each Country, we proceeded to make a violin plot comparing the Salaries of those living in different Countries. That plot did not look great, so we reverted to using box plots for comparing Races' Salary earnings. The main takeaway we received from these two plots was that the Country and Race of a person do not seem to be significant factors in determining one's salary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#EDA (Country/Race)\nSalary_cleaned %>% \n  group_by(Country, Race) %>%\n  summarize(count = n())\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'Country'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 17 x 3\n# Groups:   Country [5]\n   Country   Race             count\n   <chr>     <chr>            <int>\n 1 Australia Asian              470\n 2 Australia Australian         449\n 3 Australia White              407\n 4 Canada    Asian              452\n 5 Canada    Black              428\n 6 Canada    White              431\n 7 China     Chinese            441\n 8 China     Korean             454\n 9 China     White              438\n10 UK        Asian              328\n11 UK        Mixed              329\n12 UK        Welsh              330\n13 UK        White              327\n14 USA       African American   349\n15 USA       Asian              330\n16 USA       Hispanic           318\n17 USA       White              346\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(Salary_cleaned, aes(x = Salary, y = Country, fill = as.factor(Country))) + \n    scale_fill_manual(values = c(\"red\", \"white\", \"gold\", \"purple\", \"blue\")) +\n    geom_violin() +\n    theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](Final-Project_files/figure-pdf/unnamed-chunk-16-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nggplot(Salary_cleaned, aes(x = Salary, y = Race, color =  as.factor(Race))) +\n    geom_boxplot() +\n    theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](Final-Project_files/figure-pdf/unnamed-chunk-16-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Exploring Job Title:\n\nWhen thinking of what to explore with Job Titles, we were at first a little unsure of what to compare, since there were so many unique Job Titles in the data. We ended up simply making a table of the top 10 highest-salary jobs and the \"top 10\" lowest-salary jobs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#EDA (Job Title)\n#hrm...\nsalary_by_job_title <- Salary_cleaned %>%\n  group_by(`Job Title`) %>%\n  mutate(Average_Salary = mean(Salary)) %>%\n  distinct(Average_Salary)\n\ntop_jobs <- salary_by_job_title %>%\n  ungroup() %>%\n  arrange(desc(Average_Salary)) %>%\n  slice(1:10)\n\nworst_jobs <- salary_by_job_title %>%\n  ungroup() %>%\n  arrange(Average_Salary) %>%\n  slice(1:10)\n\ntop_jobs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 x 2\n   `Job Title`                 Average_Salary\n   <chr>                                <dbl>\n 1 Director of Data Science           204561.\n 2 Director of Human Resources        187500 \n 3 Marketing Director                 183615.\n 4 Director of Engineering            180000 \n 5 Director of Finance                175000 \n 6 Software Engineer Manager          172961.\n 7 Director of Operations             172727.\n 8 Project Engineer                   166064.\n 9 Data Scientist                     164099.\n10 Research Director                  163333.\n```\n\n\n:::\n\n```{.r .cell-code}\nworst_jobs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 x 2\n   `Job Title`                     Average_Salary\n   <chr>                                    <dbl>\n 1 Receptionist                            25000 \n 2 Delivery Driver                         28000 \n 3 Sales Associate                         30736.\n 4 Juniour HR Coordinator                  32000 \n 5 Customer Service Representative         33333.\n 6 Business Operations Analyst             35000 \n 7 HR Coordinator                          38321.\n 8 Business Development Associate          40714.\n 9 Sales Representative                    41728.\n10 Copywriter                              42500 \n```\n\n\n:::\n:::\n\n\nOne interesting thing that we could see from these tables is that Job Titles with \"Director\" and \"Engineer\" are featured frequently in the higher end of the Salary data. This could either be an insight into the types of jobs that give high Salaries, or the types of jobs that the data was scraped from. Either way, the wide range of names meant that job titles were most likely going to be largely ineffective as a predictor for our models.\n\n# Modeling:\n\nNow that we have gathered some insights about this data as well as having answered our minor questions from our exploratory analysis, we will use modeling to answer our main question.\n\nBefore we get into creating the models, we will split the salary dataset into a training and testing data frame, using a 90/10 proportion respectively.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsalary_split <- initial_split(Salary_cleaned, prop = 0.90)\ntraining <-training(salary_split)\ntesting <- testing(salary_split)\n```\n:::\n\n\nTo create a ridge regression model, we need to turn all of our datasets into numeric factors. We will get to the ridge regression model later. This is simply up here for rendering reasons.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge_salary <- Salary_cleaned %>%\n  transform(.,\n                   Race = as.numeric(as.factor(Race)),\n                   Country = as.numeric(as.factor(Country)),\n                   `Job Title` = as.numeric(as.factor(`Job Title`)),\n                   Gender = as.numeric(as.factor(Gender)))\nridge_split <- initial_split(ridge_salary, prop = .90)\ntrain <- training(ridge_split)\ntest <- testing(ridge_split)\n```\n:::\n\n\n## Multiple Linear Regression:\n\nNow that we split the data into training and testing, we will create our first model: a multiple linear regression model. A multiple linear regression model is simple, yet it can still give a good benchmark for comparisons to our other models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(Salary ~ ., data = training)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Salary ~ ., data = training)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-130031  -11632     -39   11627   64819 \n\nCoefficients:\n                                             Estimate Std. Error t value\n(Intercept)                                 29928.289  11802.594   2.536\nAge                                           -14.080    129.725  -0.109\nGenderMale                                    408.407    649.929   0.628\n`Education Level`                            6555.230    605.413  10.828\n`Job Title`Accountant                       -1241.538  14509.808  -0.086\n`Job Title`Administrative Assistant        -37338.367  19474.323  -1.917\n`Job Title`Back end Developer               29009.116  11346.786   2.557\n`Job Title`Business Analyst                 13137.112  12428.202   1.057\n`Job Title`Business Development Associate   -9708.276  14508.993  -0.669\n`Job Title`Business Development Manager     26873.222  15093.180   1.780\n`Job Title`Business Operations Analyst     -12172.564  25133.411  -0.484\n`Job Title`Content Marketing Manager        21133.828  11597.082   1.822\n`Job Title`Copywriter                       -9266.246  19474.657  -0.476\n`Job Title`Customer Service Manager        -29261.380  25166.053  -1.163\n`Job Title`Customer Service Representative  -9908.242  15090.651  -0.657\n`Job Title`Data Analyst                     55116.572  11309.428   4.874\n`Job Title`Data Engineer                    27869.128  15929.931   1.749\n`Job Title`Data Scientist                   54843.132  11337.019   4.838\n`Job Title`Delivery Driver                  -4430.745  15895.226  -0.279\n`Job Title`Digital Marketing Manager        11720.800  11702.851   1.002\n`Job Title`Digital Marketing Specialist      5068.344  12854.389   0.394\n`Job Title`Director of Data Science         60171.042  11755.169   5.119\n`Job Title`Director of Engineering          25574.474  19511.560   1.311\n`Job Title`Director of Finance              19884.510  19499.300   1.020\n`Job Title`Director of HR                    8781.104  11647.376   0.754\n`Job Title`Director of Human Resources      20154.762  19522.319   1.032\n`Job Title`Director of Marketing            24180.932  11560.182   2.092\n`Job Title`Director of Operations           12476.124  13556.848   0.920\n`Job Title`Engineer                          9539.483  25176.469   0.379\n`Job Title`Event Coordinator               -32340.228  19461.607  -1.662\n`Job Title`Financial Advisor                16927.053  15086.755   1.122\n`Job Title`Financial Analyst                18141.979  11692.480   1.552\n`Job Title`Financial Manager                44225.634  11424.797   3.871\n`Job Title`Front end Developer              22475.597  11346.619   1.981\n`Job Title`Front End Developer              18796.445  11997.842   1.567\n`Job Title`Full Stack Engineer              35102.420  11331.680   3.098\n`Job Title`Graphic Designer                  1020.743  12367.284   0.083\n`Job Title`HR Coordinator                   -9158.079  12072.138  -0.759\n`Job Title`HR Generalist                      633.249  11480.731   0.055\n`Job Title`HR Manager                        3326.472  17178.843   0.194\n`Job Title`Human Resources Coordinator      -9744.386  11736.003  -0.830\n`Job Title`Human Resources Manager          15267.205  11410.124   1.338\n`Job Title`IT Consultant                    27279.850  19485.582   1.400\n`Job Title`IT Support Specialist            -3721.373  19474.892  -0.191\n`Job Title`Juniour HR Coordinator           -5105.120  17178.421  -0.297\n`Job Title`Juniour HR Generalist             2533.346  19479.390   0.130\n`Job Title`Manager                          24358.106  19508.886   1.249\n`Job Title`Marketing Analyst                 7856.637  11414.485   0.688\n`Job Title`Marketing Coordinator             9518.554  11393.315   0.835\n`Job Title`Marketing Director               64181.876  11660.830   5.504\n`Job Title`Marketing Manager                18040.873  11333.788   1.592\n`Job Title`Marketing Specialist              7083.117  13503.677   0.525\n`Job Title`Operations Analyst               -7442.419  14084.533  -0.528\n`Job Title`Operations Coordinator           21705.957  15896.798   1.365\n`Job Title`Operations Manager               14198.842  11438.440   1.241\n`Job Title`Product Designer                  6432.888  11554.360   0.557\n`Job Title`Product Manager                  57184.910  11323.601   5.050\n`Job Title`Product Marketing Manager        27833.851  11639.769   2.391\n`Job Title`Project Coordinator               6257.842  15916.323   0.393\n`Job Title`Project Engineer                 52603.498  11359.423   4.631\n`Job Title`Project Manager                  24496.634  11936.545   2.052\n`Job Title`Receptionist                     -6410.276  11686.008  -0.549\n`Job Title`Recruiter                       -21951.130  17166.964  -1.279\n`Job Title`Research Director                50314.961  11658.754   4.316\n`Job Title`Research Scientist               43471.626  11507.443   3.778\n`Job Title`Sales Associate                  -7900.317  11370.077  -0.695\n`Job Title`Sales Director                   32680.199  11632.546   2.809\n`Job Title`Sales Executive                  -1755.312  11971.399  -0.147\n`Job Title`Sales Manager                    19714.850  11657.916   1.691\n`Job Title`Sales Representative             -6732.375  11551.173  -0.583\n`Job Title`Scientist                        16044.362  17202.574   0.933\n`Job Title`Social Media Manager               758.593  12863.367   0.059\n`Job Title`Social Media Specialist         -10199.809  25122.532  -0.406\n`Job Title`Software Developer                3055.415  11371.820   0.269\n`Job Title`Software Engineer                45585.683  11269.879   4.045\n`Job Title`Software Engineer Manager        35912.978  11365.923   3.160\n`Job Title`Training Specialist             -17175.178  19476.932  -0.882\n`Job Title`UX Designer                      17972.537  15111.272   1.189\n`Job Title`Web Developer                       -2.519  11433.755   0.000\n`Years of Experience`                        5503.175    159.973  34.401\nCountryCanada                                 400.249   1133.572   0.353\nCountryChina                                  -27.303   1461.210  -0.019\nCountryUK                                     448.525   1240.060   0.362\nCountryUSA                                   -356.912   1207.575  -0.296\nRaceAsian                                    1985.676   1618.122   1.227\nRaceAustralian                               2491.922   2091.627   1.191\nRaceBlack                                     874.105   2092.007   0.418\nRaceChinese                                   -73.100   2272.955  -0.032\nRaceHispanic                                 1811.532   1847.325   0.981\nRaceKorean                                   2453.398   2265.309   1.083\nRaceMixed                                    1573.856   2251.454   0.699\nRaceWelsh                                    -904.009   2261.541  -0.400\nRaceWhite                                    2200.761   1612.457   1.365\nSenior                                     -12053.225   1286.107  -9.372\n                                           Pr(>|t|)    \n(Intercept)                                 0.01125 *  \nAge                                         0.91357    \nGenderMale                                  0.52977    \n`Education Level`                           < 2e-16 ***\n`Job Title`Accountant                       0.93181    \n`Job Title`Administrative Assistant         0.05525 .  \n`Job Title`Back end Developer               0.01060 *  \n`Job Title`Business Analyst                 0.29054    \n`Job Title`Business Development Associate   0.50344    \n`Job Title`Business Development Manager     0.07505 .  \n`Job Title`Business Operations Analyst      0.62818    \n`Job Title`Content Marketing Manager        0.06845 .  \n`Job Title`Copywriter                       0.63423    \n`Job Title`Customer Service Manager         0.24499    \n`Job Title`Customer Service Representative  0.51148    \n`Job Title`Data Analyst                    1.13e-06 ***\n`Job Title`Data Engineer                    0.08026 .  \n`Job Title`Data Scientist                  1.35e-06 ***\n`Job Title`Delivery Driver                  0.78045    \n`Job Title`Digital Marketing Manager        0.31661    \n`Job Title`Digital Marketing Specialist     0.69338    \n`Job Title`Director of Data Science        3.17e-07 ***\n`Job Title`Director of Engineering          0.19000    \n`Job Title`Director of Finance              0.30789    \n`Job Title`Director of HR                   0.45093    \n`Job Title`Director of Human Resources      0.30193    \n`Job Title`Director of Marketing            0.03650 *  \n`Job Title`Director of Operations           0.35746    \n`Job Title`Engineer                         0.70477    \n`Job Title`Event Coordinator                0.09662 .  \n`Job Title`Financial Advisor                0.26192    \n`Job Title`Financial Analyst                0.12081    \n`Job Title`Financial Manager                0.00011 ***\n`Job Title`Front end Developer              0.04766 *  \n`Job Title`Front End Developer              0.11725    \n`Job Title`Full Stack Engineer              0.00196 ** \n`Job Title`Graphic Designer                 0.93422    \n`Job Title`HR Coordinator                   0.44811    \n`Job Title`HR Generalist                    0.95601    \n`Job Title`HR Manager                       0.84647    \n`Job Title`Human Resources Coordinator      0.40640    \n`Job Title`Human Resources Manager          0.18094    \n`Job Title`IT Consultant                    0.16157    \n`Job Title`IT Support Specialist            0.84847    \n`Job Title`Juniour HR Coordinator           0.76634    \n`Job Title`Juniour HR Generalist            0.89653    \n`Job Title`Manager                          0.21187    \n`Job Title`Marketing Analyst                0.49129    \n`Job Title`Marketing Coordinator            0.40350    \n`Job Title`Marketing Director              3.87e-08 ***\n`Job Title`Marketing Manager                0.11149    \n`Job Title`Marketing Specialist             0.59993    \n`Job Title`Operations Analyst               0.59723    \n`Job Title`Operations Coordinator           0.17217    \n`Job Title`Operations Manager               0.21453    \n`Job Title`Product Designer                 0.57772    \n`Job Title`Product Manager                 4.55e-07 ***\n`Job Title`Product Marketing Manager        0.01682 *  \n`Job Title`Project Coordinator              0.69421    \n`Job Title`Project Engineer                3.72e-06 ***\n`Job Title`Project Manager                  0.04019 *  \n`Job Title`Receptionist                     0.58334    \n`Job Title`Recruiter                        0.20106    \n`Job Title`Research Director               1.62e-05 ***\n`Job Title`Research Scientist               0.00016 ***\n`Job Title`Sales Associate                  0.48719    \n`Job Title`Sales Director                   0.00498 ** \n`Job Title`Sales Executive                  0.88343    \n`Job Title`Sales Manager                    0.09087 .  \n`Job Title`Sales Representative             0.56003    \n`Job Title`Scientist                        0.35103    \n`Job Title`Social Media Manager             0.95298    \n`Job Title`Social Media Specialist          0.68476    \n`Job Title`Software Developer               0.78818    \n`Job Title`Software Engineer               5.30e-05 ***\n`Job Title`Software Engineer Manager        0.00159 ** \n`Job Title`Training Specialist              0.37791    \n`Job Title`UX Designer                      0.23435    \n`Job Title`Web Developer                    0.99982    \n`Years of Experience`                       < 2e-16 ***\nCountryCanada                               0.72404    \nCountryChina                                0.98509    \nCountryUK                                   0.71759    \nCountryUSA                                  0.76758    \nRaceAsian                                   0.21982    \nRaceAustralian                              0.23355    \nRaceBlack                                   0.67609    \nRaceChinese                                 0.97434    \nRaceHispanic                                0.32682    \nRaceKorean                                  0.27884    \nRaceMixed                                   0.48455    \nRaceWelsh                                   0.68937    \nRaceWhite                                   0.17235    \nSenior                                      < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22450 on 5870 degrees of freedom\nMultiple R-squared:  0.8203,\tAdjusted R-squared:  0.8175 \nF-statistic: 288.2 on 93 and 5870 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nAs we can see, the most significant predictors are education level, years of experience, and senior, all of which make sense as it is logical that the more years of experience you have in a profession and the higher level of education you have, the more likely you are going to earn more money than someone who has less experience and a lesser degree. Seniority also makes sense as a senior-level position undoubtedly has more responsibilities than someone who isn't. However, we can see that several job title codes are good indicators. Jobs such as software engineer, research scientist, research director, product manager, and data scientist/analyst all appear to be very good predictors for our model. This may well be because there are simply more observations of these job titles in the data set, but all of these fields are certainly very highly-paying positions. Now, to look at the results. We can see that the model generated an R-squared value of .82, on an F-stat of 296.3, and a p-value of \\<2.2e-16, so needless to say, this is a respectable model; it is not perfect, but there is a strong positive correlation between the predictors and Salary.\n\nBefore we go any further, we should check the assumptions of our model to see if this dataset even can be fitted into a linear model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,2))\nplot(fit)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: not plotting observations with leverage one:\n  1029, 1328, 1526, 3977\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Final-Project_files/figure-pdf/unnamed-chunk-21-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nChecking the normal assumptions of linear regression, we can see that the data appears to fit to an acceptable level. The residuals vs. Fitted values graph is distributed mostly evenly from end to end, and the Q-Q Residuals plot, while both tails do slightly veer off the mean, they do at least mirror each other.\n\nNow let us fit this model into our testing data. As you can see, we bound the predicted outcomes onto the testing dataset so we can compare the predicted value to the employee's actual salary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_preds <- predict(fit, testing) %>%\n  bind_cols(testing)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNew names:\n* `` -> `...1`\n```\n\n\n:::\n\n```{.r .cell-code}\nlm_preds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 663 x 10\n     ...1   Age Gender Education L~1 Job T~2 Years~3 Salary Country Race  Senior\n    <dbl> <dbl> <chr>          <dbl> <chr>     <dbl>  <dbl> <chr>   <chr>  <dbl>\n 1 26766.    29 Female             0 Sales ~       1  25000 USA     Afri~      0\n 2 30036.    24 Male               0 Sales ~       1  25000 UK      Asian      0\n 3 29495.    30 Female             0 Sales ~       1  25000 Canada  Asian      0\n 4 29282.    30 Female             0 Sales ~       1  25000 China   White      0\n 5 25101.    21 Female             0 Sales ~       0  25000 Austra~ White      0\n 6 25101.    21 Female             0 Sales ~       0  25000 Austra~ White      0\n 7 25326.    21 Female             0 Sales ~       0  25000 China   Kore~      0\n 8 25368.    23 Female             0 Recept~       0  25000 China   White      0\n 9 22950.    25 Female             0 Sales ~       0  25000 Canada  Black      0\n10 24124.    24 Female             0 Sales ~       0  25000 UK      Asian      0\n# ... with 653 more rows, and abbreviated variable names 1: `Education Level`,\n#   2: `Job Title`, 3: `Years of Experience`\n```\n\n\n:::\n:::\n\n\nWhile the predictions are not perfect, the model does get rather close to predicting the salary of some employees, with some predictions getting even within 1000 dollars of the actual value. However, it is not perfect, so let's tune the model to see if we can improve the accuracy.\n\nLet us try to optimize the model by running a step-forward selection model to see what variables it would choose to use.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_step_forward_p(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n                                      Selection Summary                                        \n----------------------------------------------------------------------------------------------\n        Variable                               Adj.                                               \nStep           Entered           R-Square    R-Square      C(p)          AIC           RMSE       \n----------------------------------------------------------------------------------------------\n   1    Age                        0.8173      0.8149     14.8766    136597.4169    22609.9201    \n   2    `Education Level`          0.8200      0.8175    -70.5327    136511.4333    22445.6649    \n   3    `Job Title`                    NA          NA          NA             NA            NA    \n   4    `Years of Experience`          NA          NA          NA             NA            NA    \n   5    Senior                         NA          NA          NA             NA            NA    \n----------------------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\nUnsurprisingly, the summary has chosen the variables that I had highlighted in the original model. Interestingly, this model scraps Gender, Race, and Country; it does not consider them strong enough to influence the model.\n\nNow that we've figured out the ideal variables for the model, let's create a new model to see if we can improve the accuracy by removing unnecessary predictors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstep_fit <- \n    lm(Salary ~ Age + `Education Level` +\n           `Years of Experience` + Senior + \n           `Job Title`, data = training)\nsummary(step_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Salary ~ Age + `Education Level` + `Years of Experience` + \n    Senior + `Job Title`, data = training)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-129513  -11475       2   11344   64806 \n\nCoefficients:\n                                             Estimate Std. Error t value\n(Intercept)                                 32183.287  11679.385   2.756\nAge                                            -6.117    128.658  -0.048\n`Education Level`                            6517.358    602.918  10.810\n`Years of Experience`                        5500.394    159.398  34.507\nSenior                                     -12011.461   1284.469  -9.351\n`Job Title`Accountant                       -1840.322  14490.033  -0.127\n`Job Title`Administrative Assistant        -37974.794  19444.595  -1.953\n`Job Title`Back end Developer               28679.014  11333.089   2.531\n`Job Title`Business Analyst                 12301.558  12414.596   0.991\n`Job Title`Business Development Associate   -9901.037  14494.816  -0.683\n`Job Title`Business Development Manager     26115.888  15071.593   1.733\n`Job Title`Business Operations Analyst     -11773.834  25099.699  -0.469\n`Job Title`Content Marketing Manager        20473.959  11584.626   1.767\n`Job Title`Copywriter                       -9774.228  19442.392  -0.503\n`Job Title`Customer Service Manager        -29397.335  25125.735  -1.170\n`Job Title`Customer Service Representative -10227.194  15065.214  -0.679\n`Job Title`Data Analyst                     54709.254  11296.302   4.843\n`Job Title`Data Engineer                    27542.013  15921.582   1.730\n`Job Title`Data Scientist                   54418.231  11325.278   4.805\n`Job Title`Delivery Driver                  -4036.471  15883.396  -0.254\n`Job Title`Digital Marketing Manager        11403.506  11691.097   0.975\n`Job Title`Digital Marketing Specialist      4593.009  12838.143   0.358\n`Job Title`Director of Data Science         59751.930  11743.207   5.088\n`Job Title`Director of Engineering          24547.027  19480.411   1.260\n`Job Title`Director of Finance              20070.802  19471.801   1.031\n`Job Title`Director of HR                    8335.919  11635.185   0.716\n`Job Title`Director of Human Resources      18326.628  19486.263   0.940\n`Job Title`Director of Marketing            23711.257  11549.211   2.053\n`Job Title`Director of Operations           11566.978  13544.414   0.854\n`Job Title`Engineer                          9509.945  25127.956   0.378\n`Job Title`Event Coordinator               -32736.831  19442.455  -1.684\n`Job Title`Financial Advisor                16295.364  15068.647   1.081\n`Job Title`Financial Analyst                17763.123  11678.115   1.521\n`Job Title`Financial Manager                43635.738  11412.454   3.824\n`Job Title`Front end Developer              22016.991  11334.787   1.942\n`Job Title`Front End Developer              18041.921  11977.121   1.506\n`Job Title`Full Stack Engineer              34564.765  11318.901   3.054\n`Job Title`Graphic Designer                   176.921  12353.030   0.014\n`Job Title`HR Coordinator                   -9324.122  12062.302  -0.773\n`Job Title`HR Generalist                        7.556  11468.637   0.001\n`Job Title`HR Manager                        3221.066  17160.489   0.188\n`Job Title`Human Resources Coordinator     -10359.193  11722.287  -0.884\n`Job Title`Human Resources Manager          14610.811  11396.271   1.282\n`Job Title`IT Consultant                    27522.368  19463.689   1.414\n`Job Title`IT Support Specialist            -5204.361  19445.257  -0.268\n`Job Title`Juniour HR Coordinator           -5534.826  17154.220  -0.323\n`Job Title`Juniour HR Generalist              -12.790  19447.475  -0.001\n`Job Title`Manager                          24297.539  19480.880   1.247\n`Job Title`Marketing Analyst                 7350.420  11400.657   0.645\n`Job Title`Marketing Coordinator             8692.080  11375.633   0.764\n`Job Title`Marketing Director               63822.900  11644.619   5.481\n`Job Title`Marketing Manager                17517.708  11320.783   1.547\n`Job Title`Marketing Specialist              6535.768  13491.332   0.484\n`Job Title`Operations Analyst               -8245.329  14069.841  -0.586\n`Job Title`Operations Coordinator           21138.475  15885.506   1.331\n`Job Title`Operations Manager               13629.780  11422.183   1.193\n`Job Title`Product Designer                  6072.689  11535.889   0.526\n`Job Title`Product Manager                  56684.433  11310.769   5.012\n`Job Title`Product Marketing Manager        27152.087  11624.325   2.336\n`Job Title`Project Coordinator               5406.736  15897.205   0.340\n`Job Title`Project Engineer                 52135.964  11348.870   4.594\n`Job Title`Project Manager                  24122.171  11926.693   2.023\n`Job Title`Receptionist                     -7038.750  11671.316  -0.603\n`Job Title`Recruiter                       -22324.934  17147.014  -1.302\n`Job Title`Research Director                49901.967  11645.350   4.285\n`Job Title`Research Scientist               43083.422  11495.758   3.748\n`Job Title`Sales Associate                  -8388.192  11359.752  -0.738\n`Job Title`Sales Director                   32067.004  11613.304   2.761\n`Job Title`Sales Executive                  -2859.382  11955.783  -0.239\n`Job Title`Sales Manager                    19244.112  11643.177   1.653\n`Job Title`Sales Representative             -7445.820  11539.282  -0.645\n`Job Title`Scientist                        16015.931  17190.268   0.932\n`Job Title`Social Media Manager              -462.120  12841.878  -0.036\n`Job Title`Social Media Specialist         -10030.542  25097.733  -0.400\n`Job Title`Software Developer                2433.100  11361.740   0.214\n`Job Title`Software Engineer                45173.197  11258.280   4.012\n`Job Title`Software Engineer Manager        35376.316  11353.487   3.116\n`Job Title`Training Specialist             -19216.690  19443.214  -0.988\n`Job Title`UX Designer                      17462.816  15083.762   1.158\n`Job Title`Web Developer                     -371.344  11422.028  -0.033\n                                           Pr(>|t|)    \n(Intercept)                                0.005877 ** \nAge                                        0.962079    \n`Education Level`                           < 2e-16 ***\n`Years of Experience`                       < 2e-16 ***\nSenior                                      < 2e-16 ***\n`Job Title`Accountant                      0.898940    \n`Job Title`Administrative Assistant        0.050870 .  \n`Job Title`Back end Developer              0.011414 *  \n`Job Title`Business Analyst                0.321778    \n`Job Title`Business Development Associate  0.494587    \n`Job Title`Business Development Manager    0.083186 .  \n`Job Title`Business Operations Analyst     0.639028    \n`Job Title`Content Marketing Manager       0.077223 .  \n`Job Title`Copywriter                      0.615175    \n`Job Title`Customer Service Manager        0.242045    \n`Job Title`Customer Service Representative 0.497252    \n`Job Title`Data Analyst                    1.31e-06 ***\n`Job Title`Data Engineer                   0.083709 .  \n`Job Title`Data Scientist                  1.59e-06 ***\n`Job Title`Delivery Driver                 0.799403    \n`Job Title`Digital Marketing Manager       0.329402    \n`Job Title`Digital Marketing Specialist    0.720534    \n`Job Title`Director of Data Science        3.73e-07 ***\n`Job Title`Director of Engineering         0.207688    \n`Job Title`Director of Finance             0.302695    \n`Job Title`Director of HR                  0.473748    \n`Job Title`Director of Human Resources     0.347005    \n`Job Title`Director of Marketing           0.040111 *  \n`Job Title`Director of Operations          0.393138    \n`Job Title`Engineer                        0.705102    \n`Job Title`Event Coordinator               0.092277 .  \n`Job Title`Financial Advisor               0.279560    \n`Job Title`Financial Analyst               0.128298    \n`Job Title`Financial Manager               0.000133 ***\n`Job Title`Front end Developer             0.052133 .  \n`Job Title`Front End Developer             0.132027    \n`Job Title`Full Stack Engineer             0.002270 ** \n`Job Title`Graphic Designer                0.988574    \n`Job Title`HR Coordinator                  0.439555    \n`Job Title`HR Generalist                   0.999474    \n`Job Title`HR Manager                      0.851116    \n`Job Title`Human Resources Coordinator     0.376885    \n`Job Title`Human Resources Manager         0.199869    \n`Job Title`IT Consultant                   0.157404    \n`Job Title`IT Support Specialist           0.788985    \n`Job Title`Juniour HR Coordinator          0.746971    \n`Job Title`Juniour HR Generalist           0.999475    \n`Job Title`Manager                         0.212355    \n`Job Title`Marketing Analyst               0.519123    \n`Job Title`Marketing Coordinator           0.444840    \n`Job Title`Marketing Director              4.41e-08 ***\n`Job Title`Marketing Manager               0.121822    \n`Job Title`Marketing Specialist            0.628090    \n`Job Title`Operations Analyst              0.557879    \n`Job Title`Operations Coordinator          0.183347    \n`Job Title`Operations Manager              0.232811    \n`Job Title`Product Designer                0.598618    \n`Job Title`Product Manager                 5.56e-07 ***\n`Job Title`Product Marketing Manager       0.019535 *  \n`Job Title`Project Coordinator             0.733789    \n`Job Title`Project Engineer                4.44e-06 ***\n`Job Title`Project Manager                 0.043166 *  \n`Job Title`Receptionist                    0.546478    \n`Job Title`Recruiter                       0.192977    \n`Job Title`Research Director               1.86e-05 ***\n`Job Title`Research Scientist              0.000180 ***\n`Job Title`Sales Associate                 0.460293    \n`Job Title`Sales Director                  0.005776 ** \n`Job Title`Sales Executive                 0.810987    \n`Job Title`Sales Manager                   0.098420 .  \n`Job Title`Sales Representative            0.518785    \n`Job Title`Scientist                       0.351537    \n`Job Title`Social Media Manager            0.971295    \n`Job Title`Social Media Specialist         0.689422    \n`Job Title`Software Developer              0.830439    \n`Job Title`Software Engineer               6.08e-05 ***\n`Job Title`Software Engineer Manager       0.001843 ** \n`Job Title`Training Specialist             0.323022    \n`Job Title`UX Designer                     0.247024    \n`Job Title`Web Developer                   0.974065    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22450 on 5884 degrees of freedom\nMultiple R-squared:   0.82,\tAdjusted R-squared:  0.8175 \nF-statistic: 339.2 on 79 and 5884 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nUnfortunately, Rsq remained nearly the same. However, one saving grace of the tune is that we were able to slightly reduce residual standard error and increase our F-statistic, so it may not look like it at first glance, but the model is still stronger than our initial attempt, even if only slightly.\n\nNow that we've tuned our model, let us visualize the predictions.\n\nThis first plot depicts the average predicted value of Salary at each age in the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\neffect_plot(step_fit, pred = Age)\n```\n\n::: {.cell-output-display}\n![](Final-Project_files/figure-pdf/unnamed-chunk-25-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nAs we can see, this shows a very strong correlation between salary and age.\n\nThis second plot once again depicts salary vs age, but this time plots the residual values along with showing the confidence interval of which the model operates. We can see most of our residuals lie within the interval, although there are a few outliers at both ends.\n\n\n::: {.cell}\n\n```{.r .cell-code}\neffect_plot(step_fit, pred = Age, interval = TRUE, partial.residuals = TRUE)\n```\n\n::: {.cell-output-display}\n![](Final-Project_files/figure-pdf/unnamed-chunk-26-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Tree Methods:\n\nFor our second model, we want to use the power of Tree methods to see if it could give us a better answer to our main question than multiple linear regression. We will be mainly focusing on the Decision Tree method, but we will also create a Random Forest tree for comparison.\n\n### Decision Tree:\n\nAs we learned from class, we know that decision trees can mirror human decision-making more than other methods. We want to try to put this to the test to create a decision tree model based on our salary data to see if it can accurately predict an employee's salary using binary decision-making.\n\nTo begin, we will make an untidy decision tree to visualize the decision-making process the model will take to determine salary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Non-tidy way (for visualization purposes)\ntree_fit <- rpart(Salary ~., data = train)\n\nrpart.plot(tree_fit)\n```\n\n::: {.cell-output-display}\n![](Final-Project_files/figure-pdf/unnamed-chunk-27-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nWe can see from this output that Years of Experience and job titles are very influential in decision-making. To be able to print this tree without having tens of job names crowd out the actual Boolean expression, we coded the job title to be a numeric value and factorized it, so while it's a bit harder to understand what is happening, the lower a job title's value is, the less money the position makes. Back to the tree, we can see that the longer someone works, the more money they will earn, and there are no questions about what position they will hold; they will still earn more money due to their experience. However, when we go down the tree in the opposite direction (meaning an employee has less experience), their position starts to play a more pivotal role.\n\nBelow we can see the decision tree model fitted onto the testing data. We can also see the predicted values compared to the actual salary values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree_fit_2 <- rpart(Salary ~., data = training)\ntree_preds <- predict(tree_fit_2, newdata = testing) %>%\n  bind_cols(testing)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNew names:\n* `` -> `...1`\n```\n\n\n:::\n\n```{.r .cell-code}\ntree_preds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 663 x 10\n     ...1   Age Gender Education L~1 Job T~2 Years~3 Salary Country Race  Senior\n    <dbl> <dbl> <chr>          <dbl> <chr>     <dbl>  <dbl> <chr>   <chr>  <dbl>\n 1 37692.    29 Female             0 Sales ~       1  25000 USA     Afri~      0\n 2 37692.    24 Male               0 Sales ~       1  25000 UK      Asian      0\n 3 37692.    30 Female             0 Sales ~       1  25000 Canada  Asian      0\n 4 37692.    30 Female             0 Sales ~       1  25000 China   White      0\n 5 37692.    21 Female             0 Sales ~       0  25000 Austra~ White      0\n 6 37692.    21 Female             0 Sales ~       0  25000 Austra~ White      0\n 7 37692.    21 Female             0 Sales ~       0  25000 China   Kore~      0\n 8 37692.    23 Female             0 Recept~       0  25000 China   White      0\n 9 37692.    25 Female             0 Sales ~       0  25000 Canada  Black      0\n10 37692.    24 Female             0 Sales ~       0  25000 UK      Asian      0\n# ... with 653 more rows, and abbreviated variable names 1: `Education Level`,\n#   2: `Job Title`, 3: `Years of Experience`\n```\n\n\n:::\n:::\n\n\nWhile the predictions appear to be fairly accurate to the actual values, we can see that the model is not good at predicting small changes within similar records. Therefore, we need to tune for it to factor in these smaller changes into the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tidy way + tuning   \ntree_model <- decision_tree(mode = \"regression\",\n                            cost_complexity = tune(),\n                            tree_depth = tune()) %>%\n  set_engine(\"rpart\")\n\n\ndata_recipe <- recipe(Salary ~., training)\n\nwf <- workflow() %>%\n  add_recipe(data_recipe) %>%\n  add_model(tree_model)\n\ntree_grid <- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = 5)\n\ncv_samples <- vfold_cv(training)\n\ntree_tune <- wf %>%\n  tune_grid(\n    resamples = cv_samples,\n    grid = tree_grid\n  )\n\nbest_tree <- tree_tune %>%\n  select_best(metric = \"rmse\")\n\nfinal_wf <- wf %>%\n  finalize_workflow(best_tree)\n  \n\nfinal_wf %>%\n  last_fit(salary_split) %>%\n  collect_metrics() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard    9448.    Preprocessor1_Model1\n2 rsq     standard       0.969 Preprocessor1_Model1\n```\n\n\n:::\n\n```{.r .cell-code}\ntuned_tree_preds <- final_wf %>%\n  last_fit(salary_split) %>%\n  collect_predictions() %>%\n  bind_cols(testing)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNew names:\n* `Salary` -> `Salary...4`\n* `Salary` -> `Salary...11`\n```\n\n\n:::\n:::\n\n\nAs we can see from the output of the tuned decision tree above, we get an r-squared value of .958, which is an incredible accuracy considering decision trees often suffer from low predictive power. However, our RMSE value is at a staggering 10491.67, so our outliers are heavily impacting the model in a negative way, which is usually the case for Decision Trees.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntuned_tree_preds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 663 x 14\n   id           .pred  .row Salar~1 .config   Age Gender Educa~2 Job T~3 Years~4\n   <chr>        <dbl> <int>   <dbl> <chr>   <dbl> <chr>    <dbl> <chr>     <dbl>\n 1 train/test~ 26491.     8   25000 Prepro~    29 Female       0 Sales ~       1\n 2 train/test~ 25733.    13   25000 Prepro~    24 Male         0 Sales ~       1\n 3 train/test~ 26491.    34   25000 Prepro~    30 Female       0 Sales ~       1\n 4 train/test~ 26491.    38   25000 Prepro~    30 Female       0 Sales ~       1\n 5 train/test~ 25143.    49   25000 Prepro~    21 Female       0 Sales ~       0\n 6 train/test~ 25143.    53   25000 Prepro~    21 Female       0 Sales ~       0\n 7 train/test~ 25143.    57   25000 Prepro~    21 Female       0 Sales ~       0\n 8 train/test~ 25143.    76   25000 Prepro~    23 Female       0 Recept~       0\n 9 train/test~ 25143.    77   25000 Prepro~    25 Female       0 Sales ~       0\n10 train/test~ 25143.    82   25000 Prepro~    24 Female       0 Sales ~       0\n# ... with 653 more rows, 4 more variables: Salary...11 <dbl>, Country <chr>,\n#   Race <chr>, Senior <dbl>, and abbreviated variable names 1: Salary...4,\n#   2: `Education Level`, 3: `Job Title`, 4: `Years of Experience`\n```\n\n\n:::\n:::\n\n\nLooking at our predicted values now, we can see that the model is way more accurate at factoring in slight differences between similar employees. Overall, this tuned regression decision tree does a really good job of making accurate predictions.\n\n### Random Forest Tree:\n\nFor comparison, let us look at this Random Forest Tree\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_model <- rand_forest() %>% \n    set_engine(\"ranger\") %>% \n    set_mode(\"regression\")\n\n# workflow\nrf_wf <- workflow() %>% \n    add_model(rf_model) %>% \n    add_recipe(data_recipe)\n\n# fit the regression tree\nrf_fit <- rf_wf %>% fit(training)\n\n# predict\ntesting$pred <- predict(rf_fit, testing)$.pred\n\n# metrics\ntesting %>% metrics(Salary, pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard   11495.   \n2 rsq     standard       0.958\n3 mae     standard    7746.   \n```\n\n\n:::\n:::\n\n\nThe Random Forest tree did ever so slightly worse than the tuned decision tree model, but it still is very accurate at predicting salary.\n\n## Ridge Regression:\n\nWe chose ridge regression as our final model in the hopes that we could reduce the high amount of variance in our data to create an even more accurate model than our tuned Decision Tree.\n\nLet us start with a ridge model that we manually assign the penalties for. Let us use a manual penalty of 4 for the estimate. We must also center and scale all of our predictors to standardize them before we fit the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridge_recipe <- recipe(Salary ~ ., data = train) %>%\n  step_center(all_nominal_predictors()) %>%\n  step_scale(all_nominal_predictors())\n\n\nridge_model <- linear_reg(mixture = 0, penalty = .1) %>%\n  set_engine(\"glmnet\")\n\nridge_wf <- workflow() %>%\n  add_recipe(ridge_recipe) %>%\n  add_model(ridge_model) %>%\n  fit(train)\nextract_fit_parsnip(ridge_wf) %>% tidy(penalty = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 x 3\n  term                estimate penalty\n  <chr>                  <dbl>   <dbl>\n1 (Intercept)          37921.        4\n2 Age                    284.        4\n3 Gender                5999.        4\n4 Education.Level      14904.        4\n5 Job.Title             -117.        4\n6 Years.of.Experience   5046.        4\n7 Country               -294.        4\n8 Race                    67.8       4\n9 Senior               -4887.        4\n```\n\n\n:::\n:::\n\n\nFrom the output of the model, we can tell that it is not very accurate at all. The estimated values are extremely far away from zero.\n\nNow, let us try tuning the model to see if we can improve the accuracy of the ridge regression.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## TUNING\nfolds <-vfold_cv(train)\n\nmodel <- linear_reg(mixture = 0, penalty = tune()) %>%\n  set_engine(\"glmnet\")\n\ntuned_wf <- workflow() %>%\n  add_recipe(ridge_recipe) %>%\n  add_model(ridge_model)\n\nridge_grid <- grid_regular(mixture(), penalty(), levels = 10)\n\ntuned_grid <- tune_grid(tuned_wf, resamples = folds, grid = ridge_grid)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: No tuning parameters have been detected, performance will be evaluated\nusing the resamples with no tuning. Did you want to [tune()] parameters?\n```\n\n\n:::\n\n```{.r .cell-code}\ntuned_grid %>% collect_metrics() %>% filter(.metric == \"rmse\") %>% arrange(mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 6\n  .metric .estimator   mean     n std_err .config             \n  <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   28983.    10    236. Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\nThe RMSE is almost three times larger than our decision tree model. It appears this model is not accurate at all at predicting salary.\n\nBefore we make any assumptions, let us take a look at the predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntuned_grid %>% \n    select_best() %>% \n    finalize_workflow(tuned_wf, .) %>% \n    last_fit(ridge_split) %>% \n    collect_predictions()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 663 x 5\n   id                .pred  .row Salary .config             \n   <chr>             <dbl> <int>  <dbl> <chr>               \n 1 train/test split 49701.     2  25000 Preprocessor1_Model1\n 2 train/test split 49371.    12  25000 Preprocessor1_Model1\n 3 train/test split 50812.    35  25000 Preprocessor1_Model1\n 4 train/test split 49510.    36  25000 Preprocessor1_Model1\n 5 train/test split 49646.    40  25000 Preprocessor1_Model1\n 6 train/test split 41280.    47  25000 Preprocessor1_Model1\n 7 train/test split 40670.    58  25000 Preprocessor1_Model1\n 8 train/test split 42894.    62  25000 Preprocessor1_Model1\n 9 train/test split 42792.    65  25000 Preprocessor1_Model1\n10 train/test split 43316.   112  25000 Preprocessor1_Model1\n# ... with 653 more rows\n```\n\n\n:::\n:::\n\n\nOur tuned ridge regression model overestimates salary for every employee. It is now safe to say that this model is the least accurate out of the three that we have created today.\n\n# Comparison:\n\nTo compare our models, our decision tree by far did the best, as we have previously stated, but our multiple linear regression model was still respectable, being able to predict accurately within 82% of the data. Now for the ridge regression model. Our ridge regression model was not accurate even after being scaled, centered, and tuned. We are led to believe that this may have been due to the extremely large variance within the dataset.\n\n# Conclusion:\n\nIn conclusion, we were able to answer all of our questions after analyzing and modeling the data.\n\nStarting with our minor questions:\n\n-   Women do, in fact, get paid less than men; while men do have lower-paying jobs than women, on average their jobs are likely to pay less than a man's.\n\n-   Age does play a large role in how much an employee earns. experience and age go hand in hand with one another, as you are going to gain experience as you age (unless you are unemployed or start work later than the average person). Still, being older in your field almost certainly leads to better pay. We did find, however, that 60-year-olds make about the same as 50-year-olds do on average. So do not anticipate a pay raise heading into your pre-retirement years\n\n-   Having a senior-level position does indeed lead to a pay increase on average, and while we found a handful of outliers under 30, most employees in a senior-level position were older than this mark.\n\n-   Having a higher level of education does lead to a higher salary, and quite significantly so. We would hope this would be the case considering the amount of time and resources it takes to get each higher level of education.\n\n-   No, you do not need to move to another country to get a better wage. While there may be other reasons (such as benefits) to entice you to move abroad, salary should not be one of them.\n\nTo finish off this project, let us answer our main question: Can we accurately predict the salary of an employee given the predictors from the dataset?\n\nThe answer to this question is yes. Using a tuned decision tree model we were able to achieve an accuracy of 95% on our testing data. The model is not entirely perfect, but it is certainly good for the fact that it is predicting using regression, which is extremely hard to achieve good accuracy for.\n\nTo say the accuracy of our decision tree was a surprise would be an understatement. Considering the relatively small amount of variables within the data set we thought we would not be able to accurately predict salary, so to create such an accurate model was a pleasant surprise for us.\n",
    "supporting": [
      "Final-Project_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}